{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models with PyTorch\n",
    "\n",
    "This notebook is referenced from the fourth video in the [PyTorch Beginner Series](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN) by Brad Heintz on YouTube. The video focuses on the basic concepts in PyTorch that are used to handle several deep learning tasks and demonstrates how these concepts come together to make PyTorch a robust machine learning framework. You can find the notebook associated with the video [here](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple Model\n",
    "\n",
    "This model is similar to the one built in notebook-03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(torch.nn.Module):\n",
    "    \"\"\"A simple model created to set a baseline.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(TinyModel, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Setup layers and activations\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()           # converts output to probabilities\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Architecture:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "Layer `linear1`:\n",
      "Linear(in_features=100, out_features=200, bias=True)\n",
      "\n",
      "Layer `linear2`:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "tiny_model = TinyModel()\n",
    "print(f'The Model Architecture:\\n{tiny_model}\\n')\n",
    "print(f'Layer `linear1`:\\n{tiny_model.linear1}\\n')\n",
    "print(f'Layer `linear2`:\\n{tiny_model.linear2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Model Parameters ~~~\n",
      "Parameter containing:\n",
      "tensor([[ 0.0258,  0.0937,  0.0045,  ..., -0.0874,  0.0336,  0.0357],\n",
      "        [-0.0628, -0.0074, -0.0867,  ..., -0.0379,  0.0342,  0.0006],\n",
      "        [-0.0621,  0.0726, -0.0609,  ..., -0.0836,  0.0264, -0.0588],\n",
      "        ...,\n",
      "        [ 0.0368, -0.0577, -0.0352,  ...,  0.0401,  0.0558,  0.0841],\n",
      "        [-0.0010, -0.0872, -0.0848,  ...,  0.0274,  0.0201, -0.0701],\n",
      "        [-0.0210, -0.0553, -0.0559,  ..., -0.0500,  0.0179,  0.0056]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0279,  0.0237,  0.0082,  0.0206,  0.0441, -0.0085, -0.0014, -0.0226,\n",
      "         0.0264,  0.0917, -0.0581,  0.0159, -0.0744,  0.0282, -0.0757, -0.0383,\n",
      "         0.0098,  0.0349, -0.0984, -0.0992,  0.0060,  0.0340,  0.0101, -0.0866,\n",
      "         0.0721,  0.0925, -0.0286, -0.0372,  0.0245,  0.0150, -0.0073,  0.0936,\n",
      "         0.0722, -0.0122, -0.0314, -0.0612, -0.0154, -0.0513,  0.0530, -0.0103,\n",
      "        -0.0795,  0.0654, -0.0436, -0.0639,  0.0595,  0.0823,  0.0698,  0.0975,\n",
      "         0.0960, -0.0644,  0.0573, -0.0036,  0.0161, -0.0872,  0.0426,  0.0526,\n",
      "        -0.0064, -0.0996, -0.0298, -0.0789,  0.0868,  0.0707,  0.0285, -0.0509,\n",
      "        -0.0603, -0.0843,  0.0876, -0.0487, -0.0711, -0.0890, -0.0675,  0.0851,\n",
      "         0.0257,  0.0340, -0.0834,  0.0796,  0.0960,  0.0980,  0.0408,  0.0114,\n",
      "         0.0794,  0.0803,  0.0518,  0.0148, -0.0143, -0.0043,  0.0173,  0.0992,\n",
      "         0.0318, -0.0132,  0.0433,  0.0704, -0.0711, -0.0095, -0.0796, -0.0793,\n",
      "         0.0405, -0.0713,  0.0527,  0.0978, -0.0251,  0.0033, -0.0685, -0.0628,\n",
      "        -0.0158, -0.0638, -0.0089,  0.0165, -0.0691,  0.0536,  0.0756,  0.0891,\n",
      "        -0.0337,  0.0531,  0.0456, -0.0131,  0.0779, -0.0523,  0.0824,  0.0079,\n",
      "         0.0166,  0.0790, -0.0501,  0.0211,  0.0724,  0.0905,  0.0374,  0.0611,\n",
      "         0.0874,  0.0668,  0.0649,  0.0172,  0.0627,  0.0146,  0.0457, -0.0062,\n",
      "        -0.0926,  0.0160, -0.0952,  0.0051,  0.0482,  0.0630,  0.0085, -0.0976,\n",
      "        -0.0611, -0.0968, -0.0817,  0.0700, -0.0542,  0.0566, -0.0977, -0.0829,\n",
      "         0.0120, -0.0954,  0.0227, -0.0723,  0.0756,  0.0055, -0.0147, -0.0040,\n",
      "        -0.0233, -0.0900,  0.0939, -0.0572, -0.0133,  0.0007, -0.0978, -0.0384,\n",
      "        -0.0146, -0.0342, -0.0483, -0.0794,  0.0666,  0.0197, -0.0638, -0.0521,\n",
      "         0.0911, -0.0171,  0.0565, -0.0968, -0.0638, -0.0450, -0.0118,  0.0653,\n",
      "        -0.0025,  0.0727,  0.0800, -0.0573,  0.0231,  0.0380,  0.0255, -0.0208,\n",
      "         0.0497, -0.0877,  0.0517, -0.0248, -0.0629,  0.0126,  0.0476,  0.0920],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0392, -0.0690, -0.0435,  ...,  0.0626, -0.0227, -0.0029],\n",
      "        [-0.0642,  0.0328,  0.0497,  ...,  0.0283, -0.0684, -0.0549],\n",
      "        [-0.0078,  0.0105, -0.0374,  ...,  0.0345,  0.0061,  0.0190],\n",
      "        ...,\n",
      "        [-0.0434, -0.0383, -0.0110,  ...,  0.0667,  0.0439, -0.0597],\n",
      "        [ 0.0070,  0.0041, -0.0133,  ...,  0.0058, -0.0679, -0.0530],\n",
      "        [ 0.0274, -0.0384,  0.0578,  ..., -0.0381,  0.0245,  0.0189]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0268, -0.0579, -0.0289,  0.0318,  0.0236, -0.0059,  0.0217,  0.0586,\n",
      "        -0.0181,  0.0499], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Print model parameters\n",
    "print('~~~ Model Parameters ~~~')\n",
    "for param in tiny_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Parameters for `linear1` ~~~\n",
      "Parameter containing:\n",
      "tensor([[ 0.0258,  0.0937,  0.0045,  ..., -0.0874,  0.0336,  0.0357],\n",
      "        [-0.0628, -0.0074, -0.0867,  ..., -0.0379,  0.0342,  0.0006],\n",
      "        [-0.0621,  0.0726, -0.0609,  ..., -0.0836,  0.0264, -0.0588],\n",
      "        ...,\n",
      "        [ 0.0368, -0.0577, -0.0352,  ...,  0.0401,  0.0558,  0.0841],\n",
      "        [-0.0010, -0.0872, -0.0848,  ...,  0.0274,  0.0201, -0.0701],\n",
      "        [-0.0210, -0.0553, -0.0559,  ..., -0.0500,  0.0179,  0.0056]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0279,  0.0237,  0.0082,  0.0206,  0.0441, -0.0085, -0.0014, -0.0226,\n",
      "         0.0264,  0.0917, -0.0581,  0.0159, -0.0744,  0.0282, -0.0757, -0.0383,\n",
      "         0.0098,  0.0349, -0.0984, -0.0992,  0.0060,  0.0340,  0.0101, -0.0866,\n",
      "         0.0721,  0.0925, -0.0286, -0.0372,  0.0245,  0.0150, -0.0073,  0.0936,\n",
      "         0.0722, -0.0122, -0.0314, -0.0612, -0.0154, -0.0513,  0.0530, -0.0103,\n",
      "        -0.0795,  0.0654, -0.0436, -0.0639,  0.0595,  0.0823,  0.0698,  0.0975,\n",
      "         0.0960, -0.0644,  0.0573, -0.0036,  0.0161, -0.0872,  0.0426,  0.0526,\n",
      "        -0.0064, -0.0996, -0.0298, -0.0789,  0.0868,  0.0707,  0.0285, -0.0509,\n",
      "        -0.0603, -0.0843,  0.0876, -0.0487, -0.0711, -0.0890, -0.0675,  0.0851,\n",
      "         0.0257,  0.0340, -0.0834,  0.0796,  0.0960,  0.0980,  0.0408,  0.0114,\n",
      "         0.0794,  0.0803,  0.0518,  0.0148, -0.0143, -0.0043,  0.0173,  0.0992,\n",
      "         0.0318, -0.0132,  0.0433,  0.0704, -0.0711, -0.0095, -0.0796, -0.0793,\n",
      "         0.0405, -0.0713,  0.0527,  0.0978, -0.0251,  0.0033, -0.0685, -0.0628,\n",
      "        -0.0158, -0.0638, -0.0089,  0.0165, -0.0691,  0.0536,  0.0756,  0.0891,\n",
      "        -0.0337,  0.0531,  0.0456, -0.0131,  0.0779, -0.0523,  0.0824,  0.0079,\n",
      "         0.0166,  0.0790, -0.0501,  0.0211,  0.0724,  0.0905,  0.0374,  0.0611,\n",
      "         0.0874,  0.0668,  0.0649,  0.0172,  0.0627,  0.0146,  0.0457, -0.0062,\n",
      "        -0.0926,  0.0160, -0.0952,  0.0051,  0.0482,  0.0630,  0.0085, -0.0976,\n",
      "        -0.0611, -0.0968, -0.0817,  0.0700, -0.0542,  0.0566, -0.0977, -0.0829,\n",
      "         0.0120, -0.0954,  0.0227, -0.0723,  0.0756,  0.0055, -0.0147, -0.0040,\n",
      "        -0.0233, -0.0900,  0.0939, -0.0572, -0.0133,  0.0007, -0.0978, -0.0384,\n",
      "        -0.0146, -0.0342, -0.0483, -0.0794,  0.0666,  0.0197, -0.0638, -0.0521,\n",
      "         0.0911, -0.0171,  0.0565, -0.0968, -0.0638, -0.0450, -0.0118,  0.0653,\n",
      "        -0.0025,  0.0727,  0.0800, -0.0573,  0.0231,  0.0380,  0.0255, -0.0208,\n",
      "         0.0497, -0.0877,  0.0517, -0.0248, -0.0629,  0.0126,  0.0476,  0.0920],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Print parameters for `linear1`\n",
    "print('~~~ Parameters for `linear1` ~~~')\n",
    "for param in tiny_model.linear1.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Parameters for `linear2` ~~~\n",
      "Parameter containing:\n",
      "tensor([[-0.0392, -0.0690, -0.0435,  ...,  0.0626, -0.0227, -0.0029],\n",
      "        [-0.0642,  0.0328,  0.0497,  ...,  0.0283, -0.0684, -0.0549],\n",
      "        [-0.0078,  0.0105, -0.0374,  ...,  0.0345,  0.0061,  0.0190],\n",
      "        ...,\n",
      "        [-0.0434, -0.0383, -0.0110,  ...,  0.0667,  0.0439, -0.0597],\n",
      "        [ 0.0070,  0.0041, -0.0133,  ...,  0.0058, -0.0679, -0.0530],\n",
      "        [ 0.0274, -0.0384,  0.0578,  ..., -0.0381,  0.0245,  0.0189]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0268, -0.0579, -0.0289,  0.0318,  0.0236, -0.0059,  0.0217,  0.0586,\n",
      "        -0.0181,  0.0499], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Print parameters for `linear2`\n",
    "print('~~~ Parameters for `linear2` ~~~')\n",
    "for param in tiny_model.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Layer Types\n",
    "\n",
    "Some common layer types are listed below:\n",
    "\n",
    "- Linear layers - also called fully-connected layers where every input influences every output.\n",
    "- Convolutional layers - used to handle data with a high degree of spatial correlation.\n",
    "- Recurrent layers - used for sequential data by maintaining a memory using hidden states.\n",
    "- Transformers - multi-purpose network with in-built attention heads, encoders, decoders, etc.\n",
    "- Data manipulation layers\n",
    "  - Max/Average pooling layers - reduces a tensor by combining cells and assigning max/average value.\n",
    "  - Normalization layers - re-centers and normalizes the output of one layer before passing it to another.\n",
    "  - Dropout layers - randomly sets inputs to 0, encouraging sparse representations in the model.\n",
    "\n",
    "Some associated functions that are important in building a model:\n",
    "\n",
    "- Activation functions - introduces non-linearity in the model and determines if the neuron is activated.\n",
    "- Loss functions - evaluates the \"goodness\" of the model, the weights are optimized to reduce this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "tensor([[0.6985, 0.1497, 0.1617]])\n",
      "\n",
      "~~~ Weights and Bias for the Linear Layer ~~~\n",
      "Parameter containing:\n",
      "tensor([[ 0.2979,  0.3921,  0.3643],\n",
      "        [-0.1973, -0.2998, -0.3258]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.5375, -0.3426], requires_grad=True)\n",
      "\n",
      "Outputs:\n",
      "tensor([[ 0.8632, -0.5779]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a linear layer\n",
    "linear = torch.nn.Linear(3, 2)\n",
    "\n",
    "# Define inputs\n",
    "x = torch.rand(1, 3)\n",
    "print(f'Inputs:\\n{x}\\n')\n",
    "\n",
    "# Print the weights and bias\n",
    "print('~~~ Weights and Bias for the Linear Layer ~~~')\n",
    "for param in linear.parameters():\n",
    "    print(param)\n",
    "\n",
    "# Produce outputs\n",
    "y = linear(x)\n",
    "print(f'\\nOutputs:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a convolutional neural network\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super(ConvNet, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Define model architecture\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Architecture:\n",
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Inputs:\n",
      "tensor([[[[0.6148, 0.2985, 0.0058,  ..., 0.3683, 0.0458, 0.1580],\n",
      "          [0.3759, 0.2041, 0.3890,  ..., 0.5775, 0.5208, 0.6082],\n",
      "          [0.9327, 0.2717, 0.7199,  ..., 0.3205, 0.5737, 0.9072],\n",
      "          ...,\n",
      "          [0.2765, 0.1261, 0.5468,  ..., 0.7558, 0.8959, 0.3407],\n",
      "          [0.0896, 0.7106, 0.4569,  ..., 0.4056, 0.5699, 0.5253],\n",
      "          [0.6691, 0.9141, 0.1395,  ..., 0.1006, 0.2695, 0.9650]]]])\n",
      "\n",
      "Outputs:\n",
      "tensor([[-0.0847,  0.0921, -0.0056, -0.0332,  0.0778,  0.0320,  0.0618, -0.1132,\n",
      "          0.0004,  0.0945]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CNN\n",
    "conv_net = ConvNet()\n",
    "print(f'The Model Architecture:\\n{conv_net}\\n')\n",
    "\n",
    "# Define inputs\n",
    "x = torch.rand(1, 1, 32, 32)\n",
    "print(f'Inputs:\\n{x}\\n')\n",
    "\n",
    "# Produce outputs\n",
    "y = conv_net(x)\n",
    "print(f'Outputs:\\n{y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recurrent neural network with LSTM cells\n",
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        hidden_size: int,\n",
    "        vocab_size: int,\n",
    "        tagset_size: int,\n",
    "    ) -> None:\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        # Set hidden dimensions\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define word embeddings\n",
    "        self.word_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "\n",
    "        # Define LSTM cell\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "\n",
    "        # Setup a hidden layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_size, tagset_size)\n",
    "\n",
    "    def forward(self, sentence: Tensor) -> Tensor:\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indices = {\n",
      "    \"The\": 0,\n",
      "    \"dog\": 1,\n",
      "    \"ate\": 2,\n",
      "    \"the\": 3,\n",
      "    \"apple\": 4,\n",
      "    \"Everybody\": 5,\n",
      "    \"read\": 6,\n",
      "    \"that\": 7,\n",
      "    \"book\": 8\n",
      "}\n",
      "Tag Indices = {\n",
      "    \"DET\": 0,\n",
      "    \"NN\": 1,\n",
      "    \"V\": 2\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Setup training data\n",
    "train_data = [\n",
    "    ('The dog ate the apple'.split(), ['DET', 'NN', 'V', 'DET', 'NN']),\n",
    "    ('Everybody read that book'.split(), ['NN', 'V', 'DET', 'NN']),\n",
    "    ('The apple ate the book'.split(), ['DET', 'NN', 'V', 'DET', 'NN']),\n",
    "    ('Everybody read the apple'.split(), ['NN', 'V', 'DET', 'NN']),\n",
    "]\n",
    "\n",
    "# Mapping words to indices\n",
    "word_indices = {}\n",
    "for sentence, _ in train_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_indices:\n",
    "            word_indices[word] = len(word_indices)\n",
    "print(f'Word Indices = {json.dumps(word_indices, indent=4)}')\n",
    "\n",
    "# Mapping tags to indices\n",
    "tag_indices = {'DET': 0, 'NN': 1, 'V': 2}\n",
    "print(f'Tag Indices = {json.dumps(tag_indices, indent=4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(seq: list[str], indices: dict[str, int]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Converts a sequence of words to a tensor of indices based on the given mapping.\n",
    "\n",
    "    Args:\n",
    "        seq (list[str]): A list of words to be encoded.\n",
    "        indices (dict[str, int]):\\\n",
    "            A dictionary mapping words to their corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A tensor containing the indices of the words in the input sequence.\n",
    "    \"\"\"\n",
    "    idxs = [indices[word] for word in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Architecture:\n",
      "LSTMTagger(\n",
      "  (word_embeddings): Embedding(9, 6)\n",
      "  (lstm): LSTM(6, 6)\n",
      "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LSTM model\n",
    "lstm_tagger = LSTMTagger(\n",
    "    embedding_dim=6,\n",
    "    hidden_size=6,\n",
    "    vocab_size=len(word_indices),\n",
    "    tagset_size=len(tag_indices),\n",
    ")\n",
    "print(f'The Model Architecture:\\n{lstm_tagger}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [ 10/100], Loss: 1.1207\n",
      "Epoch [ 20/100], Loss: 1.1183\n",
      "Epoch [ 30/100], Loss: 1.1159\n",
      "Epoch [ 40/100], Loss: 1.1136\n",
      "Epoch [ 50/100], Loss: 1.1113\n",
      "Epoch [ 60/100], Loss: 1.1090\n",
      "Epoch [ 70/100], Loss: 1.1068\n",
      "Epoch [ 80/100], Loss: 1.1046\n",
      "Epoch [ 90/100], Loss: 1.1024\n",
      "Epoch [100/100], Loss: 1.1003\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss function and optimizer\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm_tagger.parameters(), lr=0.001)\n",
    "\n",
    "# Setup prediction collection\n",
    "evaluation_results = {}\n",
    "\n",
    "# Train the model\n",
    "N_EPOCHS = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for sentence, tags in train_data:\n",
    "        # Prepare the inputs and targets\n",
    "        lstm_tagger.zero_grad()\n",
    "        sentence_encoded = encode_sequence(sentence, word_indices)\n",
    "        targets = encode_sequence(tags, tag_indices)\n",
    "\n",
    "        # Perform forward pass\n",
    "        tag_scores = lstm_tagger(sentence_encoded)\n",
    "        predictions = tag_scores.argmax(dim=1)\n",
    "        evaluation_results[' '.join(sentence)] = dict(\n",
    "            targets=targets.numpy().tolist(),\n",
    "            predictions=predictions.numpy().tolist(),\n",
    "        )\n",
    "\n",
    "        # Compute loss and perform backpropagation\n",
    "        loss = loss_fn(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print training data\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1:3d}/{N_EPOCHS}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"The dog ate the apple\"\n",
      "    Targets     : [0, 1, 2, 0, 1]\n",
      "    Predictions : [1, 2, 2, 2, 1]\n",
      "Sentence: \"Everybody read that book\"\n",
      "    Targets     : [1, 2, 0, 1]\n",
      "    Predictions : [1, 0, 0, 1]\n",
      "Sentence: \"The apple ate the book\"\n",
      "    Targets     : [0, 1, 2, 0, 1]\n",
      "    Predictions : [1, 1, 0, 2, 2]\n",
      "Sentence: \"Everybody read the apple\"\n",
      "    Targets     : [1, 2, 0, 1]\n",
      "    Predictions : [1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get the prediction evaluation results\n",
    "for sentence, result in evaluation_results.items():\n",
    "    print(f'Sentence: \"{sentence}\"')\n",
    "    targets = result['targets']\n",
    "    predictions = result['predictions']\n",
    "    print(f'    Targets     : {targets}')\n",
    "    print(f'    Predictions : {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Model: 44.4444%\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy of the model\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for sentence, result in evaluation_results.items():\n",
    "    targets = result['targets']\n",
    "    predictions = result['predictions']\n",
    "\n",
    "    correct_predictions += (\n",
    "        (np.array(targets) == np.array(predictions)).sum()\n",
    "    )\n",
    "    total_predictions += len(predictions)\n",
    "\n",
    "accuracy_score = correct_predictions / total_predictions\n",
    "print(f'Accuracy of the Model: {(accuracy_score * 100):.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
